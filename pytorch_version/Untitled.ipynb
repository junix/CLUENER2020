{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3d67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from callback.optimizater.adamw import AdamW\n",
    "from callback.lr_scheduler import get_linear_schedule_with_warmup\n",
    "from callback.progressbar import ProgressBar\n",
    "from tools.common import seed_everything, json_to_text\n",
    "from tools.common import init_logger, logger\n",
    "\n",
    "from models.transformers import WEIGHTS_NAME, BertConfig, AlbertConfig\n",
    "from models.bert_for_ner import BertCrfForNer\n",
    "from models.albert_for_ner import AlbertCrfForNer\n",
    "from processors.utils_ner import CNerTokenizer, get_entities\n",
    "from processors.ner_seq import convert_examples_to_features\n",
    "from processors.ner_seq import ner_processors as processors\n",
    "from processors.ner_seq import collate_fn\n",
    "from metrics.ner_metrics import SeqEntityScore\n",
    "from tools.finetuning_argparse import get_argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c6a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    ## bert ernie bert_wwm bert_wwwm_ext\n",
    "    'bert': (BertConfig, BertCrfForNer, CNerTokenizer),\n",
    "    'albert': (AlbertConfig, AlbertCrfForNer, CNerTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7583e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2021 14:05:57 - WARNING - root -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.configuration_utils -   loading configuration file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/config.json\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"lstm_dropout_prob\": 0.5,\n",
      "  \"lstm_embedding_size\": 768,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   Model name '/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext' is a path or url to a directory containing tokenizer files.\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/added_tokens.json. We won't load it.\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/special_tokens_map.json. We won't load it.\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/tokenizer_config.json. We won't load it.\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   loading file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/vocab.txt\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.modeling_utils -   loading weights file /Users/junix/code/CLUENER2020/pytorch_version/outputs/skillner_output/bert/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 0\n",
      "{0: 'X', 1: 'B-skill', 2: 'I-skill', 3: 'S-skill', 4: 'O', 5: '[START]', 6: '[END]'}\n",
      "{'X': 0, 'B-skill': 1, 'I-skill': 2, 'S-skill': 3, 'O': 4, '[START]': 5, '[END]': 6}\n",
      "model_name_or_path>> /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2021 14:05:59 - INFO - root -   Training/evaluation parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<processors.utils_ner.CNerTokenizer object at 0x7f898c230810>\n"
     ]
    }
   ],
   "source": [
    "    output_dir = \"outputs/skillner_output/bert/\"\n",
    "    task_name = \"skillner\"\n",
    "    model_type = \"bert\"\n",
    "    no_cuda = True\n",
    "    local_rank = -1\n",
    "    seed = 42\n",
    "    \n",
    "    \n",
    "    adam_epsilon=1e-08\n",
    "    adv_epsilon=1.0\n",
    "    adv_name='word_embeddings'\n",
    "    cache_dir=''\n",
    "    config_name=''\n",
    "    crf_learning_rate=5e-05\n",
    "    data_dir='/Users/junix/code/CLUENER2020/pytorch_version/datasets/skillner/'\n",
    "    do_adv=False\n",
    "    do_eval=True\n",
    "    do_lower_case=True\n",
    "    do_predict=False\n",
    "    do_train=False\n",
    "    eval_all_checkpoints=False\n",
    "    eval_max_seq_length=512\n",
    "    evaluate_during_training=False\n",
    "    fp16=False\n",
    "    fp16_opt_level='O1'\n",
    "    gradient_accumulation_steps=1\n",
    "    learning_rate=3e-05\n",
    "    local_rank=-1\n",
    "    logging_steps=448\n",
    "    loss_type='ce'\n",
    "    markup='bios'\n",
    "    max_grad_norm=1.0\n",
    "    max_steps=-1\n",
    "    model_name_or_path='/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext'\n",
    "    model_type='bert'\n",
    "    no_cuda=False\n",
    "    num_train_epochs=5.0\n",
    "    output_dir='/Users/junix/code/CLUENER2020/pytorch_version/outputs/skillner_output/'\n",
    "    overwrite_cache=False\n",
    "    overwrite_output_dir=True\n",
    "    per_gpu_eval_batch_size=24\n",
    "    per_gpu_train_batch_size=24\n",
    "    predict_checkpoints=0\n",
    "    save_steps=448\n",
    "    seed=42\n",
    "    server_ip=''\n",
    "    server_port=''\n",
    "    task_name='skillner'\n",
    "    tokenizer_name=''\n",
    "    train_max_seq_length=128\n",
    "    warmup_proportion=0.1\n",
    "    weight_decay=0.01\n",
    "    \n",
    "    output_dir = output_dir + '{}'.format(model_type)\n",
    "\n",
    "    \n",
    "    time_ = time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
    "    init_logger(log_file=output_dir + f'/{model_type}-{task_name}-{time_}.log')\n",
    "\n",
    " \n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if local_rank == -1 or no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        n_gpu = 1\n",
    "        \n",
    "    print(device, n_gpu)\n",
    "    \n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training\",\n",
    "        local_rank, device, n_gpu, bool(local_rank != -1),  )\n",
    "    # Set seed\n",
    "    seed_everything(seed)\n",
    "    # Prepare NER task\n",
    " \n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "        \n",
    "    processor = processors[task_name]()\n",
    "    label_list = processor.get_labels()\n",
    "    id2label = {i: label for i, label in enumerate(label_list)}\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    print(id2label)\n",
    "    print(label2id)\n",
    "    \n",
    "    # Load pretrained model and tokenizer\n",
    "    if local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    model_type = model_type.lower()\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "    config = config_class.from_pretrained(config_name if config_name else model_name_or_path,\n",
    "                                          num_labels=num_labels, cache_dir=cache_dir if cache_dir else None, )\n",
    "    tokenizer = tokenizer_class.from_pretrained(tokenizer_name if tokenizer_name else model_name_or_path,\n",
    "                                                do_lower_case=do_lower_case,\n",
    "                                                cache_dir=cache_dir if cache_dir else None, )\n",
    "    print(\"model_name_or_path>>\",model_name_or_path)\n",
    "    model = model_class.from_pretrained(output_dir, from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "                                        config=config, cache_dir=cache_dir if cache_dir else None)\n",
    "    if local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    model.to(device)\n",
    "    logger.info(\"Training/evaluation parameters\")\n",
    "    model.eval()\n",
    "    \n",
    "    print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4f7bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example,\n",
    "                        max_seq_length,\n",
    "                        tokenizer,\n",
    "                        cls_token_at_end=False,\n",
    "                        cls_token=\"[CLS]\",\n",
    "                        cls_token_segment_id=1,\n",
    "                        sep_token=\"[SEP]\",\n",
    "                        pad_on_left=False,\n",
    "                        pad_token=0,\n",
    "                        pad_token_segment_id=0,\n",
    "                        sequence_a_segment_id=0,\n",
    "                        mask_padding_with_zero=True, ):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = tokenizer.tokenize(example)\n",
    "    print(tokens)\n",
    "    # Account for [CLS] and [SEP] with \"- 2\".\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > max_seq_length - special_tokens_count:\n",
    "        tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids:   0   0   0   0  0     0   0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens += [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    if cls_token_at_end:\n",
    "        tokens += [cls_token]\n",
    "        segment_ids += [cls_token_segment_id]\n",
    "    else:\n",
    "        tokens = [cls_token] + tokens\n",
    "        segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "    input_len = len(tokens)\n",
    "    # Zero-pad up to the sequence length.\n",
    "#     padding_length = max_seq_length - len(input_ids)\n",
    "    padding_length = 0\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "    else:\n",
    "        input_ids += [pad_token] * padding_length\n",
    "        input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        segment_ids += [pad_token_segment_id] * padding_length\n",
    "\n",
    "#     assert len(input_ids) == max_seq_length\n",
    "#     assert len(input_mask) == max_seq_length\n",
    "#     assert len(segment_ids) == max_seq_length\n",
    " \n",
    "    logger.info(\"*** Example ***\")\n",
    "    logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
    "    logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "    logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "    logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "\n",
    "    return {\n",
    "            \"input_ids\": torch.tensor([input_ids], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor([input_mask], dtype=torch.long),\n",
    "            \"labels\": None,\n",
    "            \"token_type_ids\" : torch.tensor([segment_ids], dtype=torch.long),\n",
    "            'input_lens': torch.tensor([input_len], dtype=torch.long)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b0cfd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"掌握Java编程技巧\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "742c394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2021 14:27:51 - INFO - root -   *** Example ***\n",
      "10/19/2021 14:27:51 - INFO - root -   tokens: [CLS] 掌 握 j a v a 编 程 技 巧 [SEP]\n",
      "10/19/2021 14:27:51 - INFO - root -   input_ids: 101 2958 2995 152 143 164 143 5356 4923 2825 2341 102\n",
      "10/19/2021 14:27:51 - INFO - root -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/19/2021 14:27:51 - INFO - root -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['掌', '握', 'j', 'a', 'v', 'a', '编', '程', '技', '巧']\n"
     ]
    }
   ],
   "source": [
    "inputs = convert_to_features(\n",
    "        texts,\n",
    "        max_seq_length=eval_max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        cls_token_at_end=bool(model_type in [\"xlnet\"]),\n",
    "        cls_token=tokenizer.cls_token,\n",
    "        cls_token_segment_id=2 if model_type in [\"xlnet\"] else 0,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "        pad_on_left=bool(model_type in [\"xlnet\"]),\n",
    "        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "        pad_token_segment_id=4 if model_type in ['xlnet'] else 0,\n",
    "        sequence_a_segment_id=0,\n",
    "        mask_padding_with_zero=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "66f9e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids       => tensor([[ 101, 2958, 2995,  152,  143,  164,  143, 5356, 4923, 2825, 2341,  102]])\n",
      "attention_mask  => tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "labels          => None\n",
      "token_type_ids  => tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "input_lens      => tensor([12])\n"
     ]
    }
   ],
   "source": [
    "for k,v in inputs.items():\n",
    "    print(f'{k:15.15} => {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d569af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2958, 2995,  152,  143,  164,  143, 5356, 4923, 2825, 2341,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': None, 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'input_lens': tensor([12])}\n",
      "tensor([[[-2.0797, -2.3432, -2.3849, -2.3116, 11.2052, -2.3360, -2.3596],\n",
      "         [-2.0579, -1.9937, -2.4639, -2.3908, 11.2299, -2.2850, -2.4132],\n",
      "         [-2.0484, -2.2041, -2.3940, -2.3411, 11.2064, -2.2918, -2.3900],\n",
      "         [-1.4074, 10.3601, -3.5683, -1.7622, -2.0755, -1.4275, -1.4460],\n",
      "         [-2.1695, -2.1640, 11.0063, -1.9331, -2.3384, -1.9977, -2.5267],\n",
      "         [-2.1978, -2.3501, 11.0439, -1.8484, -2.2360, -1.8547, -2.4301],\n",
      "         [-2.2374, -2.5396, 11.0483, -1.8300, -2.2744, -1.8381, -2.3970],\n",
      "         [-2.2369, -2.5577, 11.0476, -1.8914, -2.2479, -1.7714, -2.3513],\n",
      "         [-2.2325, -2.7306, 11.0206, -1.8322, -2.1705, -1.6777, -2.2657],\n",
      "         [-2.2263, -2.5670, 11.0455, -1.8614, -2.0625, -1.7280, -2.4291],\n",
      "         [-2.2787, -2.7086, 11.0124, -1.7658, -1.8710, -1.7197, -2.3593],\n",
      "         [-1.8494, -2.3123, -2.1910, -2.1123, 10.7856, -1.9982, -2.2128]]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs[0]\n",
    "    tags = model.crf.decode(logits, inputs['attention_mask'])\n",
    "    tags = tags.squeeze(0).cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a1b4fd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 4]]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "66ef131e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['skill', 2, 9]]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tags[0][1:-1]  # [CLS]XXXX[SEP]\n",
    "label_entities = get_entities(preds, id2label, markup)\n",
    "label_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0f091c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'X',\n",
       " 1: 'B-skill',\n",
       " 2: 'I-skill',\n",
       " 3: 'S-skill',\n",
       " 4: 'O',\n",
       " 5: '[START]',\n",
       " 6: '[END]'}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "817fa4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 1, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "093f10eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['skill', 2, 9]]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8b570338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skill => Java编程技巧\n"
     ]
    }
   ],
   "source": [
    "for [ner, start, end] in label_entities:\n",
    "    print(ner, \"=>\", texts[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc3870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
