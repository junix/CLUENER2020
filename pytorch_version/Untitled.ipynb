{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3d67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from callback.optimizater.adamw import AdamW\n",
    "from callback.lr_scheduler import get_linear_schedule_with_warmup\n",
    "from callback.progressbar import ProgressBar\n",
    "from tools.common import seed_everything, json_to_text\n",
    "from tools.common import init_logger, logger\n",
    "\n",
    "from models.transformers import WEIGHTS_NAME, BertConfig, AlbertConfig\n",
    "from models.bert_for_ner import BertCrfForNer\n",
    "from models.albert_for_ner import AlbertCrfForNer\n",
    "from processors.utils_ner import CNerTokenizer, get_entities\n",
    "from processors.ner_seq import convert_examples_to_features\n",
    "from processors.ner_seq import ner_processors as processors\n",
    "from processors.ner_seq import collate_fn\n",
    "from metrics.ner_metrics import SeqEntityScore\n",
    "from tools.finetuning_argparse import get_argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c6a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    ## bert ernie bert_wwm bert_wwwm_ext\n",
    "    'bert': (BertConfig, BertCrfForNer, CNerTokenizer),\n",
    "    'albert': (AlbertConfig, AlbertCrfForNer, CNerTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a7dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, task, tokenizer, data_type='train'):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "    processor = processors[task]()\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(args.data_dir, 'cached_crf-{}_{}_{}_{}'.format(\n",
    "        data_type,\n",
    "        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
    "        str(args.train_max_seq_length if data_type == 'train' else args.eval_max_seq_length),\n",
    "        str(task)))\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        if data_type == 'train':\n",
    "            examples = processor.get_train_examples(args.data_dir)\n",
    "        elif data_type == 'dev':\n",
    "            examples = processor.get_dev_examples(args.data_dir)\n",
    "        else:\n",
    "            examples = processor.get_test_examples(args.data_dir)\n",
    "        features = convert_examples_to_features(examples=examples,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                label_list=label_list,\n",
    "                                                max_seq_length=args.train_max_seq_length if data_type == 'train' \\\n",
    "                                                    else args.eval_max_seq_length,\n",
    "                                                cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "                                                pad_on_left=bool(args.model_type in ['xlnet']),\n",
    "                                                cls_token=tokenizer.cls_token,\n",
    "                                                cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "                                                sep_token=tokenizer.sep_token,\n",
    "                                                # pad on the left for xlnet\n",
    "                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n",
    "                                                )\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "    all_lens = torch.tensor([f.input_len for f in features], dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_lens, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7583e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/15/2021 09:57:28 - WARNING - root -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.configuration_utils -   loading configuration file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/config.json\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"lstm_dropout_prob\": 0.5,\n",
      "  \"lstm_embedding_size\": 768,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.tokenization_utils -   Model name '/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext' is a path or url to a directory containing tokenizer files.\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/added_tokens.json. We won't load it.\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/special_tokens_map.json. We won't load it.\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/tokenizer_config.json. We won't load it.\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.tokenization_utils -   loading file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/vocab.txt\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/15/2021 09:57:28 - INFO - models.transformers.modeling_utils -   loading weights file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 0\n",
      "{0: 'X', 1: 'B-skill', 2: 'I-skill', 3: 'S-skill', 4: 'O', 5: '[START]', 6: '[END]'}\n",
      "{'X': 0, 'B-skill': 1, 'I-skill': 2, 'S-skill': 3, 'O': 4, '[START]': 5, '[END]': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/15/2021 09:57:31 - INFO - models.transformers.modeling_utils -   Weights of BertCrfForNer not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']\n",
      "10/15/2021 09:57:31 - INFO - models.transformers.modeling_utils -   Weights from pretrained model not used in BertCrfForNer: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "10/15/2021 09:57:31 - INFO - root -   Training/evaluation parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<processors.utils_ner.CNerTokenizer object at 0x7fd3f03f39d0>\n"
     ]
    }
   ],
   "source": [
    "    output_dir = \"outputs/skillner_output/bert/\"\n",
    "    task_name = \"skillner\"\n",
    "    model_type = \"bert\"\n",
    "    no_cuda = True\n",
    "    local_rank = -1\n",
    "    seed = 42\n",
    "    \n",
    "    \n",
    "    adam_epsilon=1e-08\n",
    "    adv_epsilon=1.0\n",
    "    adv_name='word_embeddings'\n",
    "    cache_dir=''\n",
    "    config_name=''\n",
    "    crf_learning_rate=5e-05\n",
    "    data_dir='/Users/junix/code/CLUENER2020/pytorch_version/datasets/skillner/'\n",
    "    do_adv=False\n",
    "    do_eval=True\n",
    "    do_lower_case=True\n",
    "    do_predict=False\n",
    "    do_train=False\n",
    "    eval_all_checkpoints=False\n",
    "    eval_max_seq_length=512\n",
    "    evaluate_during_training=False\n",
    "    fp16=False\n",
    "    fp16_opt_level='O1'\n",
    "    gradient_accumulation_steps=1\n",
    "    learning_rate=3e-05\n",
    "    local_rank=-1\n",
    "    logging_steps=448\n",
    "    loss_type='ce'\n",
    "    markup='bios'\n",
    "    max_grad_norm=1.0\n",
    "    max_steps=-1\n",
    "    model_name_or_path='/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext'\n",
    "    model_type='bert'\n",
    "    no_cuda=False\n",
    "    num_train_epochs=5.0\n",
    "    output_dir='/Users/junix/code/CLUENER2020/pytorch_version/outputs/skillner_output/'\n",
    "    overwrite_cache=False\n",
    "    overwrite_output_dir=True\n",
    "    per_gpu_eval_batch_size=24\n",
    "    per_gpu_train_batch_size=24\n",
    "    predict_checkpoints=0\n",
    "    save_steps=448\n",
    "    seed=42\n",
    "    server_ip=''\n",
    "    server_port=''\n",
    "    task_name='skillner'\n",
    "    tokenizer_name=''\n",
    "    train_max_seq_length=128\n",
    "    warmup_proportion=0.1\n",
    "    weight_decay=0.01\n",
    "    \n",
    "    output_dir = output_dir + '{}'.format(model_type)\n",
    "\n",
    "    \n",
    "    time_ = time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
    "    init_logger(log_file=output_dir + f'/{model_type}-{task_name}-{time_}.log')\n",
    "\n",
    " \n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if local_rank == -1 or no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        n_gpu = 1\n",
    "        \n",
    "    print(device, n_gpu)\n",
    "    \n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training\",\n",
    "        local_rank, device, n_gpu, bool(local_rank != -1),  )\n",
    "    # Set seed\n",
    "    seed_everything(seed)\n",
    "    # Prepare NER task\n",
    " \n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "        \n",
    "    processor = processors[task_name]()\n",
    "    label_list = processor.get_labels()\n",
    "    id2label = {i: label for i, label in enumerate(label_list)}\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    print(id2label)\n",
    "    print(label2id)\n",
    "    \n",
    "    # Load pretrained model and tokenizer\n",
    "    if local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    model_type = model_type.lower()\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "    config = config_class.from_pretrained(config_name if config_name else model_name_or_path,\n",
    "                                          num_labels=num_labels, cache_dir=cache_dir if cache_dir else None, )\n",
    "    tokenizer = tokenizer_class.from_pretrained(tokenizer_name if tokenizer_name else model_name_or_path,\n",
    "                                                do_lower_case=do_lower_case,\n",
    "                                                cache_dir=cache_dir if cache_dir else None, )\n",
    "    model = model_class.from_pretrained(model_name_or_path, from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "                                        config=config, cache_dir=cache_dir if cache_dir else None)\n",
    "    if local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    model.to(device)\n",
    "    logger.info(\"Training/evaluation parameters\")\n",
    "    \n",
    "    print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f7bf80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InputExample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5p/zkjx21td4gg2tg04bhrxf_x00000gn/T/ipykernel_32430/1042726368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'InputExample' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "input_example = InputExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cfd47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
