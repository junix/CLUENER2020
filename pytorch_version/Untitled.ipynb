{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3d67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from callback.optimizater.adamw import AdamW\n",
    "from callback.lr_scheduler import get_linear_schedule_with_warmup\n",
    "from callback.progressbar import ProgressBar\n",
    "from tools.common import seed_everything, json_to_text\n",
    "from tools.common import init_logger, logger\n",
    "\n",
    "from models.transformers import WEIGHTS_NAME, BertConfig, AlbertConfig\n",
    "from models.bert_for_ner import BertCrfForNer\n",
    "from models.albert_for_ner import AlbertCrfForNer\n",
    "from processors.utils_ner import CNerTokenizer, get_entities\n",
    "from processors.ner_seq import convert_examples_to_features\n",
    "from processors.ner_seq import ner_processors as processors\n",
    "from processors.ner_seq import collate_fn\n",
    "from metrics.ner_metrics import SeqEntityScore\n",
    "from tools.finetuning_argparse import get_argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c6a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    ## bert ernie bert_wwm bert_wwwm_ext\n",
    "    'bert': (BertConfig, BertCrfForNer, CNerTokenizer),\n",
    "    'albert': (AlbertConfig, AlbertCrfForNer, CNerTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a7dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, task, tokenizer, data_type='train'):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "    processor = processors[task]()\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(args.data_dir, 'cached_crf-{}_{}_{}_{}'.format(\n",
    "        data_type,\n",
    "        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
    "        str(args.train_max_seq_length if data_type == 'train' else args.eval_max_seq_length),\n",
    "        str(task)))\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        if data_type == 'train':\n",
    "            examples = processor.get_train_examples(args.data_dir)\n",
    "        elif data_type == 'dev':\n",
    "            examples = processor.get_dev_examples(args.data_dir)\n",
    "        else:\n",
    "            examples = processor.get_test_examples(args.data_dir)\n",
    "        features = convert_examples_to_features(examples=examples,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                label_list=label_list,\n",
    "                                                max_seq_length=args.train_max_seq_length if data_type == 'train' \\\n",
    "                                                    else args.eval_max_seq_length,\n",
    "                                                cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "                                                pad_on_left=bool(args.model_type in ['xlnet']),\n",
    "                                                cls_token=tokenizer.cls_token,\n",
    "                                                cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "                                                sep_token=tokenizer.sep_token,\n",
    "                                                # pad on the left for xlnet\n",
    "                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n",
    "                                                )\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "    all_lens = torch.tensor([f.input_len for f in features], dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_lens, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7583e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2021 14:05:57 - WARNING - root -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.configuration_utils -   loading configuration file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/config.json\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"lstm_dropout_prob\": 0.5,\n",
      "  \"lstm_embedding_size\": 768,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   Model name '/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext' is a path or url to a directory containing tokenizer files.\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/added_tokens.json. We won't load it.\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/special_tokens_map.json. We won't load it.\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   Didn't find file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/tokenizer_config.json. We won't load it.\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   loading file /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext/vocab.txt\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.tokenization_utils -   loading file None\n",
      "10/19/2021 14:05:57 - INFO - models.transformers.modeling_utils -   loading weights file /Users/junix/code/CLUENER2020/pytorch_version/outputs/skillner_output/bert/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 0\n",
      "{0: 'X', 1: 'B-skill', 2: 'I-skill', 3: 'S-skill', 4: 'O', 5: '[START]', 6: '[END]'}\n",
      "{'X': 0, 'B-skill': 1, 'I-skill': 2, 'S-skill': 3, 'O': 4, '[START]': 5, '[END]': 6}\n",
      "model_name_or_path>> /Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2021 14:05:59 - INFO - root -   Training/evaluation parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<processors.utils_ner.CNerTokenizer object at 0x7f898c230810>\n"
     ]
    }
   ],
   "source": [
    "    output_dir = \"outputs/skillner_output/bert/\"\n",
    "    task_name = \"skillner\"\n",
    "    model_type = \"bert\"\n",
    "    no_cuda = True\n",
    "    local_rank = -1\n",
    "    seed = 42\n",
    "    \n",
    "    \n",
    "    adam_epsilon=1e-08\n",
    "    adv_epsilon=1.0\n",
    "    adv_name='word_embeddings'\n",
    "    cache_dir=''\n",
    "    config_name=''\n",
    "    crf_learning_rate=5e-05\n",
    "    data_dir='/Users/junix/code/CLUENER2020/pytorch_version/datasets/skillner/'\n",
    "    do_adv=False\n",
    "    do_eval=True\n",
    "    do_lower_case=True\n",
    "    do_predict=False\n",
    "    do_train=False\n",
    "    eval_all_checkpoints=False\n",
    "    eval_max_seq_length=512\n",
    "    evaluate_during_training=False\n",
    "    fp16=False\n",
    "    fp16_opt_level='O1'\n",
    "    gradient_accumulation_steps=1\n",
    "    learning_rate=3e-05\n",
    "    local_rank=-1\n",
    "    logging_steps=448\n",
    "    loss_type='ce'\n",
    "    markup='bios'\n",
    "    max_grad_norm=1.0\n",
    "    max_steps=-1\n",
    "    model_name_or_path='/Users/junix/code/CLUENER2020/pytorch_version/prev_trained_model/roberta_wwm_large_ext'\n",
    "    model_type='bert'\n",
    "    no_cuda=False\n",
    "    num_train_epochs=5.0\n",
    "    output_dir='/Users/junix/code/CLUENER2020/pytorch_version/outputs/skillner_output/'\n",
    "    overwrite_cache=False\n",
    "    overwrite_output_dir=True\n",
    "    per_gpu_eval_batch_size=24\n",
    "    per_gpu_train_batch_size=24\n",
    "    predict_checkpoints=0\n",
    "    save_steps=448\n",
    "    seed=42\n",
    "    server_ip=''\n",
    "    server_port=''\n",
    "    task_name='skillner'\n",
    "    tokenizer_name=''\n",
    "    train_max_seq_length=128\n",
    "    warmup_proportion=0.1\n",
    "    weight_decay=0.01\n",
    "    \n",
    "    output_dir = output_dir + '{}'.format(model_type)\n",
    "\n",
    "    \n",
    "    time_ = time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
    "    init_logger(log_file=output_dir + f'/{model_type}-{task_name}-{time_}.log')\n",
    "\n",
    " \n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if local_rank == -1 or no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        n_gpu = 1\n",
    "        \n",
    "    print(device, n_gpu)\n",
    "    \n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training\",\n",
    "        local_rank, device, n_gpu, bool(local_rank != -1),  )\n",
    "    # Set seed\n",
    "    seed_everything(seed)\n",
    "    # Prepare NER task\n",
    " \n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "        \n",
    "    processor = processors[task_name]()\n",
    "    label_list = processor.get_labels()\n",
    "    id2label = {i: label for i, label in enumerate(label_list)}\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    print(id2label)\n",
    "    print(label2id)\n",
    "    \n",
    "    # Load pretrained model and tokenizer\n",
    "    if local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    model_type = model_type.lower()\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "    config = config_class.from_pretrained(config_name if config_name else model_name_or_path,\n",
    "                                          num_labels=num_labels, cache_dir=cache_dir if cache_dir else None, )\n",
    "    tokenizer = tokenizer_class.from_pretrained(tokenizer_name if tokenizer_name else model_name_or_path,\n",
    "                                                do_lower_case=do_lower_case,\n",
    "                                                cache_dir=cache_dir if cache_dir else None, )\n",
    "    print(\"model_name_or_path>>\",model_name_or_path)\n",
    "    model = model_class.from_pretrained(output_dir, from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "                                        config=config, cache_dir=cache_dir if cache_dir else None)\n",
    "    if local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    model.to(device)\n",
    "    logger.info(\"Training/evaluation parameters\")\n",
    "    model.eval()\n",
    "    \n",
    "    print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4f7bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example,\n",
    "                        max_seq_length,\n",
    "                        tokenizer,\n",
    "                        cls_token_at_end=False,\n",
    "                        cls_token=\"[CLS]\",\n",
    "                        cls_token_segment_id=1,\n",
    "                        sep_token=\"[SEP]\",\n",
    "                        pad_on_left=False,\n",
    "                        pad_token=0,\n",
    "                        pad_token_segment_id=0,\n",
    "                        sequence_a_segment_id=0,\n",
    "                        mask_padding_with_zero=True, ):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = tokenizer.tokenize(example)\n",
    "    print(tokens)\n",
    "    # Account for [CLS] and [SEP] with \"- 2\".\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > max_seq_length - special_tokens_count:\n",
    "        tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids:   0   0   0   0  0     0   0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens += [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    if cls_token_at_end:\n",
    "        tokens += [cls_token]\n",
    "        segment_ids += [cls_token_segment_id]\n",
    "    else:\n",
    "        tokens = [cls_token] + tokens\n",
    "        segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "    input_len = len(tokens)\n",
    "    # Zero-pad up to the sequence length.\n",
    "#     padding_length = max_seq_length - len(input_ids)\n",
    "    padding_length = 0\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "    else:\n",
    "        input_ids += [pad_token] * padding_length\n",
    "        input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        segment_ids += [pad_token_segment_id] * padding_length\n",
    "\n",
    "#     assert len(input_ids) == max_seq_length\n",
    "#     assert len(input_mask) == max_seq_length\n",
    "#     assert len(segment_ids) == max_seq_length\n",
    " \n",
    "    logger.info(\"*** Example ***\")\n",
    "    logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
    "    logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "    logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "    logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "\n",
    "    return {\n",
    "            \"input_ids\": torch.tensor([input_ids], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor([input_mask], dtype=torch.long),\n",
    "            \"labels\": None,\n",
    "            \"token_type_ids\" : torch.tensor([segment_ids], dtype=torch.long),\n",
    "            'input_lens': torch.tensor([input_len], dtype=torch.long)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b0cfd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"深入理解移动端产品设计规范；\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "742c394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2021 13:56:06 - INFO - root -   *** Example ***\n",
      "10/19/2021 13:56:06 - INFO - root -   tokens: [CLS] 深 入 理 解 移 动 端 产 品 设 计 规 范 ； [SEP]\n",
      "10/19/2021 13:56:06 - INFO - root -   input_ids: 101 3918 1057 4415 6237 4919 1220 4999 772 1501 6392 6369 6226 5745 8039 102\n",
      "10/19/2021 13:56:06 - INFO - root -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "10/19/2021 13:56:06 - INFO - root -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['深', '入', '理', '解', '移', '动', '端', '产', '品', '设', '计', '规', '范', '；']\n"
     ]
    }
   ],
   "source": [
    "inputs = convert_to_features(\n",
    "        texts,\n",
    "        max_seq_length=eval_max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        cls_token_at_end=bool(model_type in [\"xlnet\"]),\n",
    "        cls_token=tokenizer.cls_token,\n",
    "        cls_token_segment_id=2 if model_type in [\"xlnet\"] else 0,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "        pad_on_left=bool(model_type in [\"xlnet\"]),\n",
    "        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "        pad_token_segment_id=4 if model_type in ['xlnet'] else 0,\n",
    "        sequence_a_segment_id=0,\n",
    "        mask_padding_with_zero=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fa43d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids       => tensor([[ 101, 3918, 1057, 4415, 6237, 4919, 1220, 4999,  772, 1501, 6392, 6369,\n",
      "         6226, 5745, 8039,  102]])\n",
      "attention_mask  => tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "labels          => None\n",
      "token_type_ids  => tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "input_lens      => tensor([16])\n"
     ]
    }
   ],
   "source": [
    "for k,v in inputs.items():\n",
    "    print(f'{k:15.15} => {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d569af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 3918, 1057, 4415, 6237, 4919, 1220, 4999,  772, 1501, 6392, 6369,\n",
      "         6226, 5745, 8039,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': None, 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'input_lens': tensor([16])}\n",
      "tensor([[[-2.0799, -2.3608, -2.4109, -2.3249, 11.1324, -2.2533, -2.4345],\n",
      "         [-2.1101, -2.0772, -2.4351, -2.3491, 11.0841, -2.2168, -2.5134],\n",
      "         [-2.0462, -2.2469, -2.3876, -2.3154, 11.0574, -2.2193, -2.4432],\n",
      "         [-2.0812, -1.9366, -2.5212, -2.3091, 11.1660, -2.1782, -2.5151],\n",
      "         [-2.0829, -2.2295, -2.3969, -2.3073, 11.1514, -2.1942, -2.4915],\n",
      "         [-1.4453, 10.1065, -2.9470, -1.9491, -2.2120, -1.4968, -1.7068],\n",
      "         [-2.1523, -1.7321, 10.6140, -1.8609, -2.3031, -2.2402, -2.5576],\n",
      "         [-1.9857, -1.9156, 10.8755, -1.7866, -2.3202, -1.9112, -2.6826],\n",
      "         [-2.1551, -1.4362, 10.9340, -1.8617, -2.6223, -2.0206, -2.6432],\n",
      "         [-2.1417, -2.5684, 10.9880, -1.6375, -2.3688, -1.8367, -2.3907],\n",
      "         [-2.1016, -2.3340, 11.0266, -1.7039, -2.2599, -1.8487, -2.4772],\n",
      "         [-2.1172, -2.5878, 11.0314, -1.6939, -2.1087, -1.7857, -2.4149],\n",
      "         [-2.1031, -2.3169, 10.9404, -1.7938, -2.0132, -1.7119, -2.5949],\n",
      "         [-2.1035, -2.6398, 10.9040, -1.7862, -1.9934, -1.7108, -2.3982],\n",
      "         [-2.1050, -2.3837, -2.3066, -2.3218, 11.1218, -2.2140, -2.3927],\n",
      "         [-1.8469, -2.3869, -2.4275, -2.1502, 10.8339, -1.9339, -2.2628]]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(inputs)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs[0]\n",
    "    print(logits)\n",
    "    tags = model.crf.decode(logits, inputs['attention_mask'])\n",
    "    tags = tags.squeeze(0).cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a1ed8a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4]]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "33263cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['skill', 4, 12]]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tags[0][1:-1]  # [CLS]XXXX[SEP]\n",
    "label_entities = get_entities(preds, id2label, markup)\n",
    "label_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4f98b783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'X',\n",
       " 1: 'B-skill',\n",
       " 2: 'I-skill',\n",
       " 3: 'S-skill',\n",
       " 4: 'O',\n",
       " 5: '[START]',\n",
       " 6: '[END]'}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "119880a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 4]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8604e791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['skill', 4, 12]]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4e662cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skill => 移动端产品设计规范\n"
     ]
    }
   ],
   "source": [
    "for [ner, start, end] in label_entities:\n",
    "    print(ner, \"=>\", texts[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d322291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
